{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:3.0em\">\n",
    "    Wikipedia Scrape – Tony Nominees\n",
    "    <br>\n",
    "</span> \n",
    "\n",
    "<i> Some neat smooth functions for grabbing data from Wikipedia. There are some inadequacies, skip down to section 6 to read what they are.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import Functions and Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:35:57.346425Z",
     "start_time": "2019-01-23T10:35:56.485615Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import display_html\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Function to get tables from url\n",
    "\n",
    "<pre>\n",
    "get_tables_from_url(url)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:35:58.210843Z",
     "start_time": "2019-01-23T10:35:58.206112Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_tables_from_url(url):\n",
    "    \"\"\"\n",
    "    Gets a list of tables from an input url.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Request the document\n",
    "    r = requests.get(url)\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    \n",
    "    table_classes = {\"class\": [\"wikitable\"]}\n",
    "    tables = soup.findAll(\"table\", table_classes)\n",
    "    \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:21:35.061745Z",
     "start_time": "2019-01-23T10:21:35.056236Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Function to get df from tables\n",
    "<pre>\n",
    "get_df_from_table(table_in)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:00.306674Z",
     "start_time": "2019-01-23T10:36:00.294685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_df_from_table(table_in):\n",
    "    \"\"\"\n",
    "    returns a structured table for a beautiful soup table\n",
    "    \"\"\"\n",
    "    \n",
    "    ### SET UP REGEX\n",
    "    \n",
    "    re_1 = \"^\\d{4}\"\n",
    "    re_2 = \"\\d{1,2}(th|rd|st|nd) Tony Awards\"\n",
    "\n",
    "\n",
    "    # JUMP INTO IT!\n",
    "    parsed_table_data=[]\n",
    "    rows = table_in.findAll('tr')\n",
    "    \n",
    "    row_1 = rows[0]\n",
    "    children = row_1.findChildren(recursive=False)\n",
    "    r_n = len(children)\n",
    "\n",
    "    parsed_table_data = []\n",
    "    for row in rows:\n",
    "        children = row.findChildren(recursive=False)\n",
    "\n",
    "        row_text = []\n",
    "        for child in children:\n",
    "\n",
    "            clean_text = child.text\n",
    "\n",
    "            # We can sort each row as follows:\n",
    "            if re.findall(re_2,clean_text):\n",
    "                string  = clean_text\n",
    "                \n",
    "                # get the year\n",
    "                yr = re.match(re_1,string).group()\n",
    "                # split the string\n",
    "                string_2 = string.split(yr)[1]\n",
    "                # get the tony year\n",
    "                t_yr = re.match(re_2,string_2).group()\n",
    "                \n",
    "   \n",
    "\n",
    "                # Send the cleaned text back to the loop\n",
    "                clean_text = yr +' | ' + t_yr\n",
    "                row_text.append(clean_text)\n",
    "                \n",
    "                continue\n",
    "\n",
    "            #This is to discard reference/citation links\n",
    "            clean_text = clean_text.split('&#91;')[0]\n",
    "            #This is to clean the header row of the sort icons\n",
    "            clean_text = clean_text.split('&#160;')[-1]\n",
    "            # This discards demarcation of winner of pulitzer prize\n",
    "            clean_text = clean_text.replace('†','')\n",
    "\n",
    "            clean_text = clean_text.strip()\n",
    "            row_text.append(clean_text)\n",
    "            \n",
    "\n",
    "        if '|' in row_text[0]:\n",
    "            cell_1 = row_text[0]\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            if 'cell_1' not in dir():\n",
    "                cell_1 = 'nothing'\n",
    "            row_text.insert(0, cell_1)\n",
    "\n",
    "        if len(row_text) >r_n:\n",
    "            # removes first item, we don't want it...\n",
    "            row_text.pop(0)\n",
    "\n",
    "        if len(row_text) <r_n:\n",
    "            x = row_text[-1]\n",
    "            # How many items are you missing? (diff)\n",
    "            diff = r_n - len(row_text)\n",
    "            for i in range(diff):\n",
    "                # for each item missing, append\n",
    "                row_text.append(x)\n",
    "\n",
    "        parsed_table_data.append(row_text)\n",
    "    \n",
    "    ## ONTO PART II\n",
    "    headers = parsed_table_data.pop(0)\n",
    "    df = pd.DataFrame(parsed_table_data, columns=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get Musicals\n",
    "Scrape that wikipedia stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:06.269681Z",
     "start_time": "2019-01-23T10:36:06.266309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Tony_Award_for_Best_Musical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:07.147265Z",
     "start_time": "2019-01-23T10:36:06.779678Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tables = get_tables_from_url(url)\n",
    "df=pd.DataFrame()\n",
    "for table in tables:\n",
    "    sub_df = get_df_from_table(table)\n",
    "    df = df.append(sub_df,ignore_index=True)\n",
    "    \n",
    "name_root = url.split('/')[-1]\n",
    "df_name = 'Wikipedia_scrape_' + name_root + '.csv'\n",
    "df.to_csv(df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get Plays\n",
    "Scrape that wikipedia stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:08.266451Z",
     "start_time": "2019-01-23T10:36:08.261198Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Tony_Award_for_Best_Play'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:08.897823Z",
     "start_time": "2019-01-23T10:36:08.568307Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tables = get_tables_from_url(url)\n",
    "df=pd.DataFrame()\n",
    "for table in tables:\n",
    "    sub_df = get_df_from_table(table)\n",
    "    df = df.append(sub_df,ignore_index=True)\n",
    "    \n",
    "name_root = url.split('/')[-1]\n",
    "df_name = 'Wikipedia_scrape_' + name_root + '.csv'\n",
    "df.to_csv(df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get Revival of a Musical\n",
    "Scrape that wikipedia stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:09.638318Z",
     "start_time": "2019-01-23T10:36:09.635369Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Tony_Award_for_Best_Revival_of_a_Musical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:11.756420Z",
     "start_time": "2019-01-23T10:36:11.474871Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tables = get_tables_from_url(url)\n",
    "df=pd.DataFrame()\n",
    "for table in tables:\n",
    "    sub_df = get_df_from_table(table)\n",
    "    df = df.append(sub_df,ignore_index=True)\n",
    "    \n",
    "name_root = url.split('/')[-1]\n",
    "df_name = 'Wikipedia_scrape_' + name_root + '.csv'\n",
    "df.to_csv(df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get Revival of Plays\n",
    "Scrape that wikipedia stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:12.614750Z",
     "start_time": "2019-01-23T10:36:12.611514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Tony_Award_for_Best_Revival_of_a_Play'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:36:13.620035Z",
     "start_time": "2019-01-23T10:36:13.314339Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tables = get_tables_from_url(url)\n",
    "df=pd.DataFrame()\n",
    "for table in tables:\n",
    "    sub_df = get_df_from_table(table)\n",
    "    df = df.append(sub_df,ignore_index=True)\n",
    "    \n",
    "name_root = url.split('/')[-1]\n",
    "df_name = 'Wikipedia_scrape_' + name_root + '.csv'\n",
    "df.to_csv(df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Finished!\n",
    "Done! Each is in its own df.\n",
    "\n",
    "<hr>\n",
    "<i>\n",
    "    Maybe make a single df with a column representing the category of a tony nominee? Also, would be nice to include which ones are winners. This can be done in the next iteration.  \n",
    "    </i>\n",
    "    <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load df\n",
    "<i>\n",
    "If you want to load the files you just downloaded... \n",
    "</i>\n",
    "<pre>\n",
    "get_files(dir_in = os.getcwd(), csv_in = True):\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:43:10.665190Z",
     "start_time": "2019-01-23T10:43:10.660545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files(dir_in = os.getcwd(), csv_in = True):\n",
    "    \"\"\"\n",
    "    returns a list of paths of files\n",
    "    option to include csvs\n",
    "    requires you load os\n",
    "    \"\"\"\n",
    "    files = [x for x in os.listdir(dir_in)]\n",
    "    file_paths = [os.path.abspath(x) for x in os.listdir(dir_in)]\n",
    "    csv_paths = [x for x in file_paths if x.endswith('.csv')]\n",
    "    \n",
    "    if csv_in is False:\n",
    "        return file_paths\n",
    "    else:\n",
    "        return csv_paths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T10:43:41.402602Z",
     "start_time": "2019-01-23T10:43:41.376078Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file is here!\n",
      "                     Year         Musical  \\\n",
      "0  1949 | 3rd Tony Awards   Kiss Me, Kate   \n",
      "1  1950 | 4th Tony Awards   South Pacific   \n",
      "2  1951 | 5th Tony Awards  Guys and Dolls   \n",
      "3  1952 | 6th Tony Awards  The King and I   \n",
      "4  1953 | 7th Tony Awards  Wonderful Town   \n",
      "\n",
      "                                  Book              Music  \\\n",
      "0               Bella & Samuel Spewack        Cole Porter   \n",
      "1  Oscar Hammerstein II & Joshua Logan    Richard Rodgers   \n",
      "2            Abe Burrows & Jo Swerling      Frank Loesser   \n",
      "3                 Oscar Hammerstein II    Richard Rodgers   \n",
      "4      Jerome Chodorov & Joseph Fields  Leonard Bernstein   \n",
      "\n",
      "                        Lyrics  \n",
      "0                  Cole Porter  \n",
      "1                  Hammerstein  \n",
      "2                Frank Loesser  \n",
      "3                  Hammerstein  \n",
      "4  Betty Comden & Adolph Green  \n",
      "the file you want isn't here\n",
      "the file you want isn't here\n",
      "the file you want isn't here\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csvs = get_files()\n",
    "for x in csvs:\n",
    "    if x.endswith('Wikipedia_scrape_Tony_Award_for_Best_Musical.csv'):\n",
    "        df = pd.read_csv(x,index_col=0)\n",
    "        print ('file is here!')\n",
    "        print(df.head(5))\n",
    "    else:\n",
    "        print ('the file you want isn\\'t here')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
