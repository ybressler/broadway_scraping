{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:3.0em\">\n",
    "    Wikipedia Scrape – Tony Nominees\n",
    "    <br>\n",
    "</span> \n",
    "\n",
    "<i> There's a lot here. All of it good! </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages + Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:56:19.221278Z",
     "start_time": "2019-01-20T18:56:19.016391Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def getLinks_tagged_fast(url, tag):\n",
    "    r = requests.get(url)\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    links = []\n",
    "    # set the opening of each link to be...\n",
    "    tag = tag\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(tag)}):\n",
    "        links.append(link.get('href'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:22:44.523893Z",
     "start_time": "2019-01-20T16:22:44.518817Z"
    }
   },
   "source": [
    "## Get Wikipedia Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:56:22.096904Z",
     "start_time": "2019-01-20T18:56:21.385199Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display_html\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:56:22.633554Z",
     "start_time": "2019-01-20T18:56:22.116545Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Tony_Award_for_Best_Musical'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc,'lxml')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Solve on Granular Level\n",
    "_Solve without function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T17:09:43.221887Z",
     "start_time": "2019-01-20T17:09:43.184985Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table_classes = {\"class\": [\"wikitable\"]}\n",
    "tables = soup.findAll(\"table\", table_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T17:09:44.087303Z",
     "start_time": "2019-01-20T17:09:44.082179Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table = tables[1]\n",
    "parsed_table_data=[]\n",
    "rows = table.findAll('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Set up Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T17:09:59.181161Z",
     "start_time": "2019-01-20T17:09:59.176924Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use regex to extract year\n",
    "# I keep the regex example for reference\n",
    "string = '19504th Tony Awards'\n",
    "re_1 = \"^\\d{4}\"\n",
    "re_2 = \"\\d{1,2}th Tony Awards\"\n",
    "yr = re.findall(re_1,string)[0]\n",
    "string_2 = string.split(yr)[1]\n",
    "t_yr = re.findall(re_2,string_2)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Use Regex on Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:02:13.089645Z",
     "start_time": "2019-01-20T18:02:13.079744Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parsed_table_data = []\n",
    "row_1 = rows[0]\n",
    "children = row_1.findChildren(recursive=False)\n",
    "r_n = len(children)\n",
    "\n",
    "for row in rows:\n",
    "    children = row.findChildren(recursive=False)\n",
    "    \n",
    "    row_text = []\n",
    "    for child in children:\n",
    "        \n",
    "        clean_text = child.text\n",
    "        \n",
    "        # We can sort each row as follows:\n",
    "        if re.findall(re_1,clean_text):\n",
    "            string  = clean_text\n",
    "            # get the year\n",
    "            yr = re.findall(re_1,string)[0]\n",
    "            # split the string\n",
    "            string_2 = string.split(yr)[1]\n",
    "            # get the tony year\n",
    "            t_yr = re.findall(re_2,string_2)[0]\n",
    "            \n",
    "            # Send the cleaned text back to the loop\n",
    "            clean_text = yr +' | ' + t_yr\n",
    "            row_text.append(clean_text)\n",
    "            continue\n",
    "\n",
    "        #This is to discard reference/citation links\n",
    "        clean_text = clean_text.split('&#91;')[0]\n",
    "        #This is to clean the header row of the sort icons\n",
    "        clean_text = clean_text.split('&#160;')[-1]\n",
    "        # This discards demarcation of winner of pulitzer prize\n",
    "        clean_text = clean_text.replace('†','')\n",
    "        \n",
    "        clean_text = clean_text.strip()\n",
    "        row_text.append(clean_text)\n",
    "    \n",
    "    if '|' in row_text[0]:\n",
    "        cell_1 = row_text[0]\n",
    "        continue\n",
    "    else:\n",
    "        row_text.insert(0,cell_1)   \n",
    "    \n",
    "    if len(row_text) >r_n:\n",
    "        row_text.pop(0)\n",
    "\n",
    "    if len(row_text) <r_n:\n",
    "        x = row_text[-1]\n",
    "        # How many items are you missing? (diff)\n",
    "        diff = r_n - len(row_text)\n",
    "        for i in range(diff):\n",
    "            # for each item missing, append\n",
    "            row_text.append(x)\n",
    "        \n",
    "    parsed_table_data.append(row_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:02:14.642100Z",
     "start_time": "2019-01-20T18:02:14.638402Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year', 'Musical', 'Book', 'Music', 'Lyrics']\n"
     ]
    }
   ],
   "source": [
    "# Get headers\n",
    "headers = parsed_table_data.pop(0)\n",
    "# Remove the first insertion\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Create a Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T17:53:48.760985Z",
     "start_time": "2019-01-20T17:53:48.756913Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a df\n",
    "df = pd.DataFrame(parsed_table_data, columns=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random trick to check if a variable exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:14:32.235546Z",
     "start_time": "2019-01-20T18:14:32.228291Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_1 = 7\n"
     ]
    }
   ],
   "source": [
    "if 'var_1' not in dir():\n",
    "    var_1 = 3\n",
    "    print('var_1 does not exist\\nsetting var_1 = 3')\n",
    "else:\n",
    "    print('var_1 = {}'.format(var_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T17:54:12.828549Z",
     "start_time": "2019-01-20T17:54:12.825058Z"
    }
   },
   "source": [
    "## Solve with Iterator\n",
    "\n",
    "_Solve with a function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:56:38.076589Z",
     "start_time": "2019-01-20T18:56:38.062425Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_df_from_table(table_in):\n",
    "    \"\"\"\n",
    "    returns a structured table for a beautiful soup table\n",
    "    \"\"\"\n",
    "    \n",
    "    ### SET UP REGEX\n",
    "    \n",
    "    re_1 = \"^\\d{4}\"\n",
    "    re_2 = \"\\d{1,2}(th|rd|st|nd) Tony Awards\"\n",
    "\n",
    "\n",
    "    # JUMP INTO IT!\n",
    "    parsed_table_data=[]\n",
    "    rows = table_in.findAll('tr')\n",
    "    \n",
    "    row_1 = rows[0]\n",
    "    children = row_1.findChildren(recursive=False)\n",
    "    r_n = len(children)\n",
    "\n",
    "    parsed_table_data = []\n",
    "    for row in rows:\n",
    "        children = row.findChildren(recursive=False)\n",
    "\n",
    "        row_text = []\n",
    "        for child in children:\n",
    "\n",
    "            clean_text = child.text\n",
    "\n",
    "            # We can sort each row as follows:\n",
    "            if re.findall(re_2,clean_text):\n",
    "                string  = clean_text\n",
    "                \n",
    "                # get the year\n",
    "                yr = re.match(re_1,string).group()\n",
    "                # split the string\n",
    "                string_2 = string.split(yr)[1]\n",
    "                # get the tony year\n",
    "                t_yr = re.match(re_2,string_2).group()\n",
    "                \n",
    "   \n",
    "\n",
    "                # Send the cleaned text back to the loop\n",
    "                clean_text = yr +' | ' + t_yr\n",
    "                row_text.append(clean_text)\n",
    "                \n",
    "                continue\n",
    "\n",
    "            #This is to discard reference/citation links\n",
    "            clean_text = clean_text.split('&#91;')[0]\n",
    "            #This is to clean the header row of the sort icons\n",
    "            clean_text = clean_text.split('&#160;')[-1]\n",
    "            # This discards demarcation of winner of pulitzer prize\n",
    "            clean_text = clean_text.replace('†','')\n",
    "\n",
    "            clean_text = clean_text.strip()\n",
    "            row_text.append(clean_text)\n",
    "            \n",
    "\n",
    "        if '|' in row_text[0]:\n",
    "            cell_1 = row_text[0]\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            if 'cell_1' not in dir():\n",
    "                cell_1 = 'nothing'\n",
    "            row_text.insert(0, cell_1)\n",
    "\n",
    "        if len(row_text) >r_n:\n",
    "            # removes first item, we don't want it...\n",
    "            row_text.pop(0)\n",
    "\n",
    "        if len(row_text) <r_n:\n",
    "            x = row_text[-1]\n",
    "            # How many items are you missing? (diff)\n",
    "            diff = r_n - len(row_text)\n",
    "            for i in range(diff):\n",
    "                # for each item missing, append\n",
    "                row_text.append(x)\n",
    "\n",
    "        parsed_table_data.append(row_text)\n",
    "    \n",
    "    ## ONTO PART II\n",
    "    headers = parsed_table_data.pop(0)\n",
    "    df = pd.DataFrame(parsed_table_data, columns=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to List of Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:56:39.160361Z",
     "start_time": "2019-01-20T18:56:39.120408Z"
    }
   },
   "outputs": [],
   "source": [
    "table_classes = {\"class\": [\"wikitable\"]}\n",
    "tables = soup.findAll(\"table\", table_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:56:40.074283Z",
     "start_time": "2019-01-20T18:56:40.032940Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "for table in tables:\n",
    "    sub_df = get_df_from_table(table)\n",
    "    df = df.append(sub_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:56:40.924507Z",
     "start_time": "2019-01-20T18:56:40.872920Z"
    }
   },
   "source": [
    "## Save as a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:58:58.283652Z",
     "start_time": "2019-01-20T18:58:58.272061Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'Wikipedia_scrape_Tony_nominees.csv'\n",
    "df.to_csv(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
